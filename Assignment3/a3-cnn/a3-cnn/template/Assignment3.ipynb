{"cells":[{"cell_type":"markdown","metadata":{"id":"LS7T20gWaEkg"},"source":["# Machine Vision - Assignment 3: CNNs in PyTorch\n","\n","---\n","\n","Prof. Dr. Markus Enzweiler, Esslingen University of Applied Sciences\n","\n","markus.enzweiler@hs-esslingen.de\n","\n","---\n","\n","### This is the third assignment for the \"Machine Vision\" lecture.\n","It covers:\n","* training and finetuning CNNs for traffic sign recognition\n","* working with public benchmark datasets ([German Traffic Sign Recognition Benchmark](https://benchmark.ini.rub.de/gtsrb_news.html))\n","\n","**Make sure that \"GPU\" is selected in Runtime -> Change runtime type**\n","\n","To successfully complete this assignment, it is assumed that you already have some experience in Python and numpy. You can either use [Google Colab](https://colab.research.google.com/) for free with a private (dedicated) Google account (recommended) or a local Jupyter installation.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"RH3vu3HiqaZS"},"source":["## Preparations and Imports\n"]},{"cell_type":"markdown","metadata":{"id":"XJOOn6oO8X3C"},"source":["### Package Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPQaSW4f8aLt"},"outputs":[],"source":["# Package Path (this needs to be adapted)\n","packagePath = \"./\" # local\n","if 'google.colab' in str(get_ipython()):\n","  packagePath = \"/content/drive/MyDrive/a3-cnn/a3-cnn/template\"   # Colab"]},{"cell_type":"markdown","metadata":{"id":"MFz0s31SxNyP"},"source":["### Import important libraries (you should probably start with these lines all the time ...)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPcN62DgZ6Gg"},"outputs":[],"source":["# os, glob, time, logging\n","import os, glob, time, logging\n","\n","# NumPy\n","import numpy as np\n","\n","# OpenCV\n","import cv2\n","\n","# Matplotlib\n","import matplotlib.pyplot as plt\n","# make sure we show all plots directly below each cell\n","%matplotlib inline\n","\n","# PyTorch\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# Some Colab specific packages\n","if 'google.colab' in str(get_ipython()):\n","  # image display\n","  from google.colab.patches import cv2_imshow\n","\n","  # torchinfo\n","  %pip install torchinfo"]},{"cell_type":"markdown","metadata":{"id":"ANRNsyDPTm9x"},"source":["\n","### Some helper functions that we will need"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQ5AIQr1TsHU"},"outputs":[],"source":["def my_imshow(image, windowTitle=None, size=20, depth=3):\n","  '''\n","  Displays an image and differentiates between Google Colab and a local Python installation.\n","\n","  Args:\n","    image: The image to be displayed\n","\n","  Returns:\n","    -\n","  '''\n","\n","  if 'google.colab' in str(get_ipython()):\n","    print(windowTitle)\n","    cv2_imshow(image)\n","  else:\n","    if (size):\n","      (h, w) = image.shape[:2]\n","      aspectRatio = float(h)/w\n","      figsize=(size, size * aspectRatio)\n","      plt.figure(figsize=figsize)\n","\n","    if (windowTitle):\n","      plt.title(windowTitle)\n","\n","    if (depth == 1):\n","      plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n","    elif (depth == 3):\n","      plt.imshow(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n","    else:\n","      plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ybKcetvDlt-o"},"source":["### In Google Colab only:\n","Mount the Google Drive associated with your Google account. You will have to click the authorization link and enter the obtained authorization code here in Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88rhI7gsltLi"},"outputs":[],"source":["# Mount Google Drive\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"LRE7wONf8ez8"},"source":["### PyTorch Trainer and Test Class\n","\n","In the project package, I have provided a Python file `torchHelpers.py` that contains two classes `Trainer` and `Tester`. These classes contain the neural network training loop and test code, similar to what you have already seen in the tutorial.\n","\n","The classes can be used as follows (see the documentation of the individual classes):\n","\n","```\n","# Train a neural network model\n","# create a trainer\n","trainer = Trainer(model, lossFunction, optimizer, device, logLevel=logging.INFO)\n","# train the model\n","trainer.train(trainLoader, valLoader, numberOfEpochs)\n","\n","# Test a neural network model\n","# create a tester\n","tester = Tester(model, device, logLevel=logging.INFO)\n","# test the model\n","tester.test(testLoader)\n","```\n","\n","Error and accuracy metrics are available after training / testing via `trainer.metrics` and `tester.metrics`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzeMhjWs9v-5"},"outputs":[],"source":["import sys\n","sys.path.append(packagePath)\n","print(\"If the import does not work, most likely your 'packagePath' is not set correctly!\")\n","print(f'packagePath: {packagePath}')\n","\n","from torchHelpers import Trainer, Tester\n","\n","help(Trainer)"]},{"cell_type":"markdown","metadata":{"id":"p_uHg7EecLVh"},"source":["## Exercise 1 - Traffic Sign Classification using Convolutional Neural Networks in PyTorch (10 points)\n","\n","In this exercise you will train a convolutional neural network using PyTorch on the [German Traffic Sign Recognition Benchmark](https://benchmark.ini.rub.de/gtsrb_news.html) dataset. There will be no previous feature transform, i.e. the raw pixel values are the input to the neural network.\n","\n","**For that, you can use exactly the same procedure as in the previous assignment, with the same datasets, loss function, optimizer, hyperparameters, `Trainer` and `Tester` class. You will just have to define a different network model.**"]},{"cell_type":"markdown","metadata":{"id":"ODQh70XnjUtE"},"source":["### Automatically select the best available device (**PROVIDED**)\n","\n","**In Colab: Make sure that \"GPU\" is selected in Runtime -> Change runtime type**\n","\n","You should have a GPU device available, e.g.:\n","\n","```\n","Using device: cuda\n","Tesla T4\n","```\n","\n","We will transfer our model and data to this device later on in the Assignment. PyTorch takes care of all the particular device handling automatically, i.e. we will not have to explicitly deal with CUDA / MPS."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdbikbaijUtE"},"outputs":[],"source":["# Check the devices that we have available and prefer CUDA over MPS and CPU\n","def autoselectDevice(verbose=1):\n","\n","    # default: CPU\n","    device = torch.device('cpu')\n","\n","    if torch.cuda.is_available():\n","        # CUDA\n","        device = torch.device('cuda')\n","    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n","        # MPS (acceleration on Apple M-series SoC)\n","        device = torch.device('mps')\n","\n","    if verbose:\n","        print('Using device:', device)\n","\n","    # Additional Info when using cuda\n","    if verbose and device.type == 'cuda':\n","        print(torch.cuda.get_device_name(0))\n","\n","    return device\n","\n","# We transfer our model and data later to this device. If this is a GPU\n","# PyTorch will take care of everything automatically.\n","device = autoselectDevice(verbose=1)\n"]},{"cell_type":"markdown","metadata":{"id":"8Wr5nnGBjUtE"},"source":["### Getting familiar with the GTSRB dataset (**PROVIDED**)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y26VgFLnjUtE"},"outputs":[],"source":["# GTSRB is available as standard dataset in PyTorch. Nice :)\n","\n","# Data is loaded and processed in batches of 'batchSize' images\n","batchSize = 24\n","\n","# We can add a chain of transforms that is applied to the original data, e.g.\n","#    resize all images to the same dimensions, e.g. 64x64 pixels\n","#    convert (batch of) images to a tensor\n","#    normalize pixel values (to 0-1)\n","\n","transform = transforms.Compose(\n","    [transforms.Resize((64, 64)), # resize to 64x64 pixels\n","     transforms.ToTensor()        # convert to tensor. This will also normalize pixels to 0-1\n","     ])\n","\n","# We construct several DataLoaders that take care of loading, storing, caching, pre-fetching the dataset.\n","# We will have one DataLoader for training, validation and test data.\n","\n","# Training data\n","trainSet = torchvision.datasets.GTSRB(root='./data', split='train',\n","                                      download=True, transform=transform)\n","trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,\n","                                          shuffle=True, pin_memory=True, num_workers=2)\n","numTrainSamples = len(trainSet)\n","\n","# Validation and test data\n","# GTSRB only provides a single test set. To create a validation and test set,\n","# we split the original GTSRB test set into two parts. The validation set is\n","# used to tune performance during training. The test set is only used AFTER\n","# training to evaluation the final performance.\n","\n","gtsrbTestSet = torchvision.datasets.GTSRB(root='./data', split='test',\n","                                          download=True, transform=transform)\n","\n","# Split the original GTSRB test data into 75% used for validation and 25% used for testing\n","# We do not need to shuffle the data, as we are processing every validation / test image exactly once\n","length75Percent = int(0.75 * len(gtsrbTestSet))\n","length25Percent = len(gtsrbTestSet) - length75Percent\n","lengths = [length75Percent, length25Percent]\n","valSet, testSet = torch.utils.data.random_split(gtsrbTestSet, lengths)\n","valLoader = torch.utils.data.DataLoader(valSet, batch_size=batchSize,\n","                                        shuffle=False, pin_memory=True, num_workers=2)\n","numValSamples = len(valSet)\n","\n","testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize,\n","                                         shuffle=False, pin_memory=True, num_workers=2)\n","numTestSamples = len(testSet)\n","\n","# Available traffic sign classes in the dataset\n","classes = [\n","    \"Speed limit (20km/h)\",\n","    \"Speed limit (30km/h)\",\n","    \"Speed limit (50km/h)\",\n","    \"Speed limit (60km/h)\",\n","    \"Speed limit (70km/h)\",\n","    \"Speed limit (80km/h)\",\n","    \"End of speed limit (80km/h)\",\n","    \"Speed limit (100km/h)\",\n","    \"Speed limit (120km/h)\",\n","    \"No passing\",\n","    \"No passing for vehicles over 3.5 metric tons\",\n","    \"Right-of-way at the next intersection\",\n","    \"Priority road\",\n","    \"Yield\",\n","    \"Stop\",\n","    \"No vehicles\",\n","    \"Vehicles over 3.5 metric tons prohibited\",\n","    \"No entry\",\n","    \"General caution\",\n","    \"Dangerous curve to the left\",\n","    \"Dangerous curve to the right\",\n","    \"Double curve\",\n","    \"Bumpy road\",\n","    \"Slippery road\",\n","    \"Road narrows on the right\",\n","    \"Road work\",\n","    \"Traffic signals\",\n","    \"Pedestrians\",\n","    \"Children crossing\",\n","    \"Bicycles crossing\",\n","    \"Beware of ice/snow\",\n","    \"Wild animals crossing\",\n","    \"End of all speed and passing limits\",\n","    \"Turn right ahead\",\n","    \"Turn left ahead\",\n","    \"Ahead only\",\n","    \"Go straight or right\",\n","    \"Go straight or left\",\n","    \"Keep right\",\n","    \"Keep left\",\n","    \"Roundabout mandatory\",\n","    \"End of no passing\",\n","    \"End of no passing by vehicles over 3.5 metric tons\",\n","]\n","\n","numClasses = len(classes)"]},{"cell_type":"markdown","metadata":{"id":"fM6is9qFjUtE"},"source":["### Print dataset statistics (**PROVIDED**)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vRi9LJpjUtE"},"outputs":[],"source":["print(\"Dataset Statistics\")\n","print(f\"  # of training samples:   {numTrainSamples}\")\n","print(f\"  # of validation samples: {numValSamples}\")\n","print(f\"  # of test samples:       {numTestSamples}\")\n","print(f\"  # of different classes:  {numClasses}\")"]},{"cell_type":"markdown","metadata":{"id":"CMNSQPYzjUtE"},"source":["### Visualize the data (**PROVIDED**)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EyFxa7AjUtE"},"outputs":[],"source":["# Visualize a random batch of data from the data set\n","\n","def imshow(img):\n","    npimg = img.cpu().numpy() # make sure image is in host memory\n","\n","    # normalize to 0-1 for visualization\n","    minPixel = np.min(npimg)\n","    maxPixel = np.max(npimg)\n","    npimg = npimg - minPixel\n","    npimg = npimg / (maxPixel-minPixel)\n","\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","\n","numRows = 8\n","\n","# get a single random batch of training images\n","dataIter = iter(trainLoader)\n","images, labels = next(dataIter)\n","\n","# print labels\n","for i in range( batchSize // numRows ):\n","    print('\\n'.join(f'Image {j:2d}: {classes[labels[j]]:5s}' for j in range((i*numRows), (i*numRows)+numRows)))\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images, nrow=numRows))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6CBKeJ_jUtE"},"outputs":[],"source":["print(\"Dataset Statistics\")\n","print(f\"  # of training samples:   {numTrainSamples}\")\n","print(f\"  # of validation samples: {numValSamples}\")\n","print(f\"  # of test samples:       {numTestSamples}\")\n","print(f\"  # of different classes:  {numClasses}\")"]},{"cell_type":"markdown","metadata":{"id":"D8zBN61-cr-2"},"source":["### Define a Convolutional Neural Network (**add your code here**)\n","\n","We want to design a Convolutional Neural Network as seen in this [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). The overall structure of our network class is the same as before for the case of multilayer perceptrons. We will however use additional layers (convolutional and pooling layers).\n","\n","We will need the following layers (input to output):\n","\n","* The input will be the raw pixel values, i.e. 64x64x3 (**not flattened as for the multilayer perceptron**)\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 16 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 16 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) pooling layer with 2x2 kernels\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 32 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 32 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) pooling layer with 2x2 kernels\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 64 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) convolutional layer with 64 output channels (different features), 3x3 kernels and `padding='same'`\n","* 1 [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) pooling layer with 2x2 kernels\n","* 1 [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) fully connected layer with 64 neurons\n","* The output layer is a [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) fully connected layer with `numClasses` neurons, one neuron per class\n","* All neurons except for neurons in the output layer and pooling layers should have [`torch.nn.functional.leaky_relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.leaky_relu.html#torch.nn.functional.leaky_relu) activation functions\n","* **Important: The output layer must not have any activation function. It will be automatically applied in the loss computation (softmax activation for CrossEntropy loss, as seen in the lecture).**\n","\n","\n","The model summary should look similar to:\n","```\n","==========================================================================================\n","Layer (type (var_name))                  Output Shape              Param #\n","==========================================================================================\n","ConvNet (ConvNet)                        [1, 43]                   --\n","├─Conv2d (conv1)                         [1, 16, 64, 64]           448\n","├─Conv2d (conv2)                         [1, 16, 64, 64]           2,320\n","├─MaxPool2d (pool)                       [1, 16, 32, 32]           --\n","├─Conv2d (conv3)                         [1, 32, 32, 32]           4,640\n","├─Conv2d (conv4)                         [1, 32, 32, 32]           9,248\n","├─MaxPool2d (pool)                       [1, 32, 16, 16]           --\n","├─Conv2d (conv5)                         [1, 64, 16, 16]           18,496\n","├─Conv2d (conv6)                         [1, 64, 16, 16]           36,928\n","├─MaxPool2d (pool)                       [1, 64, 8, 8]             --\n","├─Linear (fc1)                           [1, 64]                   262,208\n","├─Linear (fc2)                           [1, 43]                   2,795\n","==========================================================================================\n","Total params: 337,083\n","Trainable params: 337,083\n","Non-trainable params: 0\n","Total mult-adds (M): 40.01\n","==========================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 1.84\n","Params size (MB): 1.35\n","Estimated Total Size (MB): 3.23\n","==========================================================================================\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHXXZdufc4JY"},"outputs":[],"source":["###### YOUR CODE GOES HERE ######\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ConvNet(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n","    self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n","    self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n","    self.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","    self.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n","    self.fc1 = nn.Linear(in_features=64*8*8, out_features=64)\n","    self.fc2 = nn.Linear(in_features=64, out_features=43)\n","\n","  def forward(self,x):\n","    x = F.leaky_relu(self.conv1(x))\n","    x = F.leaky_relu(self.conv2(x))\n","    x = self.pool(x)\n","    x = F.leaky_relu(self.conv3(x))\n","    x = F.leaky_relu(self.conv4(x))\n","    x = self.pool(x)\n","    x = F.leaky_relu(self.conv5(x))\n","    x = F.leaky_relu(self.conv6(x))\n","    x = self.pool(x)\n","    x = x.view(-1, 64*8*8)  # Flatten the tensor\n","    x = F.leaky_relu(self.fc1(x))\n","    x = self.fc2(x)  # No activation function for the output layer\n","    return x\n","#################################\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfpnfmhyd_XP"},"outputs":[],"source":["# Instantiate the CNN\n","cnn = ConvNet()\n","from torchinfo import summary\n","summary(cnn, input_size=(1, 3, 64, 64), row_settings=[\"var_names\"])"]},{"cell_type":"markdown","metadata":{"id":"7Uycu6V2c61c"},"source":["### Train your CNN on the training set and use the validation set to report performance in each iteration (**add your code here**).\n","\n","To train our network, we will first have to define a loss function, an optimizer and hyperparameters that control the training process:\n","\n","* [`torch.optim.AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?highlight=adamw#torch.optim.AdamW) is used as an optimizer with default parameters except for the learning rate which is set to `lr=3e-4`.\n","* [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) is used as loss function. Note, that the softmax activation is applied during loss computation, as stated in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n","* The number of training epochs is 15\n","\n","The overall training should take about 20 seconds per epoch (**on a GPU**, depending on what GPU is assigned). Reported accuracies on the training (validation) data should be approx. 99.5% (93%) after 15 training epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQ_pnGavdQgu"},"outputs":[],"source":["###### YOUR CODE GOES HERE ######\n","lr = 3e-4\n","model = cnn\n","param_optimizer = model.parameters()\n","lossFunction = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(param_optimizer, lr=lr)\n","\n","trainer = Trainer(model, lossFunction, optimizer, device)\n","trainer.train(trainLoader, valLoader, 15)\n","#################################"]},{"cell_type":"markdown","metadata":{"id":"k1mUOulQjUtF"},"source":["### Visualize the behavior of the loss and accuracy (**add your code here**)\n","\n","Using the data available in `trainer.metrics`, create the following two plots:\n","* Training loss and validation loss as a function of epochs.  \n","* Training accuracy and validation accuracy as a function of epochs.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBVAfXLPjUtF"},"outputs":[],"source":["##### YOUR CODE GOES HERE ######\n","trainLoss = trainer.metrics[\"epochTrainLoss\"]\n","valLoss = trainer.metrics[\"epochValLoss\"]\n","trainacc = trainer.metrics[\"epochTrainAccuracy\"]\n","trainValacc = trainer.metrics[\"epochValAccuracy\"]\n","\n","# Number of epochs\n","epochs = range(1, len(trainLoss) + 1)\n","\n","# Plotting training and validation loss\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, trainLoss, 'b', label='Training loss')\n","plt.plot(epochs, valLoss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plotting training and validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, trainacc, 'b', label='Training accuracy')\n","plt.plot(epochs, trainValacc, 'r', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","################################"]},{"cell_type":"markdown","metadata":{"id":"cEddvnVbjUtF"},"source":["### Run your network on some images to get predictions (**PROVIDED**)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUHScbgkjUtF"},"outputs":[],"source":["# Run some batches of unseen test data through the network and visualize its predictions\n","numBatches = 2\n","numRows = 8\n","\n"," # Iterator through the test DataLoader\n","dataIter = iter(testLoader)\n","\n","# for each batch\n","for batch in range(numBatches):\n","\n","    # get images and ground truth labels\n","    images, labels = next(dataIter)\n","\n","    # push to the device used\n","    images, labels = images.to(device), labels.to(device)\n","\n","    # forward pass of the batch of images\n","    outputs = cnn(images)\n","\n","    # find the index of the class with the largest output\n","    _, predictedLabels         = torch.max(outputs, 1)\n","\n","    # print labels and outputs\n","    countCorrect = 0\n","    for i in range( batchSize // numRows ):\n","        for j in range((i*numRows), (i*numRows)+numRows):\n","            print(f'Image {j:2d} - Label: {classes[labels[j]]:5s} | Prediction: {classes[predictedLabels[j]]:5s}')\n","            if labels[j] == predictedLabels[j]:\n","                countCorrect = countCorrect + 1\n","\n","    print(f\"\\n{(countCorrect / batchSize) * 100.0:.2f}% of test images correctly classified\")\n","\n","    # show images\n","    imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"JsaL4PVqjUtF"},"source":["### Evaluate the performance on the unseen test data set.  (**add your code here**)\n","\n","Use the proviced `Tester` class (see above) and test your trained network on the unseen test set available via `testLoader`. Accuracy on the unseen test dataset should be approx. 90%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCvzYCt3jUtF"},"outputs":[],"source":["###### YOUR CODE GOES HERE ######\n","# Test a neural network model\n","# create a tester\n","tester = Tester(model, device)\n","# test the model\n","tester.test(testLoader)\n","\n","#################################"]},{"cell_type":"markdown","metadata":{"id":"EPoQDqxqn_D6"},"source":["## Exercise 2 - Finetuning of CNNs for Traffic Sign Classification\n","\n","In this exercise you will finetune a pre-trained CNN using PyTorch on the [German Traffic Sign Recognition Benchmark](https://benchmark.ini.rub.de/gtsrb_news.html) dataset. Finetuning / transfer-learning involves using a pre-trained CNN as a backbone for feature extraction and replacing the final classification layers with custom classification layers (remember the encoder vs. decoder discussion in our lecture). PyTorch comes with a large set of [pre-trained models](https://pytorch.org/vision/stable/models.html) that have been trained on ImageNet. We will finetune a MobileNet V2 to our dataset.\n","\n","\n","Some more links for finetuning / transfer learning:\n","* [PyTorch transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n","* [CS231n lecture notes on transfer learning](https://cs231n.github.io/transfer-learning/)"]},{"cell_type":"markdown","metadata":{"id":"TaK0tcqcn_D6"},"source":["### Redefine the dataset (**PROVIDED**)\n","\n","Changes:\n","* 256x256 pixel input (that's what the pretrained models use)\n","* Decreased the batch size to 16 to save GPU VRAM\n","* Added the data transforms which were used when MobileNet was trained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXgWsErzn_D6"},"outputs":[],"source":["# GTSRB is available as standard dataset in PyTorch. Nice :)\n","\n","# Data is loaded and processed in batches of 'batchSize' images\n","batchSize = 16\n","\n","# We can add a chain of transforms that is applied to the original data, e.g.\n","#    resize all images to the same dimensions, e.g. 256x256 pixels\n","#    convert (batch of) images to a tensor\n","#    normalize pixel values (to 0-1)\n","\n","transform = transforms.Compose(\n","    [transforms.Resize((256, 256)), # resize to 256x256 pixels\n","     transforms.ToTensor(),         # convert to tensor. This will also normalize pixels to 0-1\n","\n","     # add the transforms which were used when MobileNet was trained\n","     torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2.transforms(antialias=True)\n","\n","     ])\n","\n","# We construct several DataLoaders that take care of loading, storing, caching, pre-fetching the dataset.\n","# We will have one DataLoader for training, validation and test data.\n","\n","# Training data\n","trainSet = torchvision.datasets.GTSRB(root='./data', split='train',\n","                                      download=True, transform=transform)\n","trainLoader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize,\n","                                          shuffle=True, pin_memory=True, num_workers=2)\n","numTrainSamples = len(trainSet)\n","\n","# Validation and test data\n","# GTSRB only provides a single test set. To create a validation and test set,\n","# we split the original GTSRB test set into two parts. The validation set is\n","# used to tune performance during training. The test set is only used AFTER\n","# training to evaluation the final performance.\n","\n","gtsrbTestSet = torchvision.datasets.GTSRB(root='./data', split='test',\n","                                          download=True, transform=transform)\n","\n","# Split the original GTSRB test data into 75% used for validation and 25% used for testing\n","# We do not need to shuffle the data, as we are processing every validation / test image exactly once\n","length75Percent = int(0.75 * len(gtsrbTestSet))\n","length25Percent = len(gtsrbTestSet) - length75Percent\n","lengths = [length75Percent, length25Percent]\n","valSet, testSet = torch.utils.data.random_split(gtsrbTestSet, lengths)\n","valLoader = torch.utils.data.DataLoader(valSet, batch_size=batchSize,\n","                                        shuffle=False, pin_memory=True, num_workers=2)\n","numValSamples = len(valSet)\n","\n","testLoader = torch.utils.data.DataLoader(testSet, batch_size=batchSize,\n","                                         shuffle=False, pin_memory=True, num_workers=2)\n","numTestSamples = len(testSet)\n","\n","# Available traffic sign classes in the dataset\n","classes = [\n","    \"Speed limit (20km/h)\",\n","    \"Speed limit (30km/h)\",\n","    \"Speed limit (50km/h)\",\n","    \"Speed limit (60km/h)\",\n","    \"Speed limit (70km/h)\",\n","    \"Speed limit (80km/h)\",\n","    \"End of speed limit (80km/h)\",\n","    \"Speed limit (100km/h)\",\n","    \"Speed limit (120km/h)\",\n","    \"No passing\",\n","    \"No passing for vehicles over 3.5 metric tons\",\n","    \"Right-of-way at the next intersection\",\n","    \"Priority road\",\n","    \"Yield\",\n","    \"Stop\",\n","    \"No vehicles\",\n","    \"Vehicles over 3.5 metric tons prohibited\",\n","    \"No entry\",\n","    \"General caution\",\n","    \"Dangerous curve to the left\",\n","    \"Dangerous curve to the right\",\n","    \"Double curve\",\n","    \"Bumpy road\",\n","    \"Slippery road\",\n","    \"Road narrows on the right\",\n","    \"Road work\",\n","    \"Traffic signals\",\n","    \"Pedestrians\",\n","    \"Children crossing\",\n","    \"Bicycles crossing\",\n","    \"Beware of ice/snow\",\n","    \"Wild animals crossing\",\n","    \"End of all speed and passing limits\",\n","    \"Turn right ahead\",\n","    \"Turn left ahead\",\n","    \"Ahead only\",\n","    \"Go straight or right\",\n","    \"Go straight or left\",\n","    \"Keep right\",\n","    \"Keep left\",\n","    \"Roundabout mandatory\",\n","    \"End of no passing\",\n","    \"End of no passing by vehicles over 3.5 metric tons\",\n","]\n","\n","numClasses = len(classes)"]},{"cell_type":"markdown","metadata":{"id":"RUvygoiyn_D7"},"source":["### Neural Network Model Definition (**add your code here**)\n","\n","To finetune an existing CNN in PyTorch the procedure is as follows:\n","* Create a custom network class in Python that subclasses [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) and contains `__init__()` and `forward()`.\n","* Add an instance variable, e.g. `self.model` to your class and initialize this with your pre-trained model (see `model_ft` in the [PyTorch transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html))\n","* We will use [MobileNet V2](https://pytorch.org/vision/stable/models/mobilenetv2.html)\n","* Read through the [documentation of MobileNet](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v2.html#torchvision.models.mobilenet_v2) and select pre-trained weights (network parameters). Most of the time, only ImageNet is available. Some models provide pre-trained weights on different datasets. Also, make sure to figure out, what the input size of your pre-trained model is. In the case of MobileNet, the input images should have 256x256 pixels. When preparing the dataloaders (see above), check your `transforms` so that they scale the image to the correct size. This has already been prepared in the dataloader code above.  Since the input images are larger now, you might want to modify your batch size to not run out of GPU memory. Here, we will use a batch of 16 images instead of 24 in the previous assignment.\n","* Before using the pre-trained models, one must preprocess the image (resize with right resolution/interpolation, apply  transforms, rescale the values etc). There is no standard way to do this as it depends on how a given model was trained. It can vary across model families, variants or even weight versions. Using the correct preprocessing method is critical and failing to do so may lead to decreased accuracy or incorrect outputs. All the necessary information for the transforms of each pre-trained model is provided on its weights documentation. To simplify inference, TorchVision bundles the necessary preprocessing transforms into each model weight. These are accessible via the `weight.transforms` attribute. This has already been prepared in the dataloader code above.\n","* Every pre-trained model consists of a backbone network for feature extraction and some (fully connected) classifier layers on top. To replace the ImageNet classification layers present in our-pretrained model (instance variables or the model), we will need to find their variable name. This unfortunately involves a bit of guessing or browsing through the [model source code](https://pytorch.org/vision/stable/_modules/torchvision/models/mobilenetv2.html). Usually, the variables are named either `classifier` or `fc` (fully connected). There is some code below to list all available layer names. Typically, the classification layer is the last one.\n","* Set the  `classifier` or `fc` parameter of your model to your new classification layer. If you only want to use a single classification layer, this should be a [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer. If you want to use a stack of layers as classifier, they need to be added to an [`nn.Sequential` container](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). If the pre-trained model already has a stack of layers as classifier, it is usually enough to replace the final layer with your new classification layer. In any case, you will need to know the output dimension of the previous layer (same as the input dimension of your new classification layer). This is available as `in_features` of the final original classifier layer depending on the particular model. Save this first before replacing the classifier layer.\n","* Re-train your whole model  \n","\n","\n","In this assignment, we will replace the final MobileNet classification layer using a single [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer with `numClasses` output neurons.\n","\n","\n","**Setup your network!**\n","\n","**Important: The output layer must not have any activation function. It will be automatically applied in the loss computation (softmax activation for CrossEntropy loss, as seen in the lecture).**\n"]},{"cell_type":"markdown","metadata":{"id":"4uPmK9xpn_D7"},"source":["**Some hints**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NOg-iu1n_D7"},"outputs":[],"source":["# Load a pre-trained MobileNet model with ImageNet weights\n","backbone = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V2)\n","\n","# Print the layer names to guess what the classification layer(s) are named\n","print(\"Mobilenet layer names:\")\n","for name, child in backbone.named_children():\n","    print(f\"     {name}\")\n","\n","# Looks like \"classifier\" is the correct layer here.\n","# Let's find out the input dimension of \"classifier\"\n","\n","print(f\"The Mobilenet final classification layer has an input dimension of {backbone.classifier[1].in_features}.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rIyYIqSxn_EA"},"source":["**torchinfo.summary() might also be useful here**\n","\n","It shows a nn.Sequential() container layer named `classifier` which contains the 1000 neuron classification layer (1000 = number of ImageNet output classes) as second element.\n","\n","```\n","MobileNetV2 (MobileNetV2)                     [1, 1000]                 --\n","├─Sequential (features)                       [1, 1280, 8, 8]           --\n","│    └─Conv2dNormActivation (0)               [1, 32, 128, 128]         --\n","│    │    └─Conv2d (0)                        [1, 32, 128, 128]         864\n","│    │    └─BatchNorm2d (1)                   [1, 32, 128, 128]         64\n","│    │    └─ReLU6 (2)                         [1, 32, 128, 128]         --\n","│    └─InvertedResidual (1)                   [1, 16, 128, 128]         --\n","│    │    └─Sequential (conv)                 [1, 16, 128, 128]         896\n","│    └─InvertedResidual (2)                   [1, 24, 64, 64]           --\n","│    │    └─Sequential (conv)                 [1, 24, 64, 64]           5,136\n","│    └─InvertedResidual (3)                   [1, 24, 64, 64]           --\n","│    │    └─Sequential (conv)                 [1, 24, 64, 64]           8,832\n","│    └─InvertedResidual (4)                   [1, 32, 32, 32]           --\n","│    │    └─Sequential (conv)                 [1, 32, 32, 32]           10,000\n","│    └─InvertedResidual (5)                   [1, 32, 32, 32]           --\n","│    │    └─Sequential (conv)                 [1, 32, 32, 32]           14,848\n","│    └─InvertedResidual (6)                   [1, 32, 32, 32]           --\n","│    │    └─Sequential (conv)                 [1, 32, 32, 32]           14,848\n","│    └─InvertedResidual (7)                   [1, 64, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 64, 16, 16]           21,056\n","│    └─InvertedResidual (8)                   [1, 64, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 64, 16, 16]           54,272\n","│    └─InvertedResidual (9)                   [1, 64, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 64, 16, 16]           54,272\n","│    └─InvertedResidual (10)                  [1, 64, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 64, 16, 16]           54,272\n","│    └─InvertedResidual (11)                  [1, 96, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 96, 16, 16]           66,624\n","│    └─InvertedResidual (12)                  [1, 96, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 96, 16, 16]           118,272\n","│    └─InvertedResidual (13)                  [1, 96, 16, 16]           --\n","│    │    └─Sequential (conv)                 [1, 96, 16, 16]           118,272\n","│    └─InvertedResidual (14)                  [1, 160, 8, 8]            --\n","│    │    └─Sequential (conv)                 [1, 160, 8, 8]            155,264\n","│    └─InvertedResidual (15)                  [1, 160, 8, 8]            --\n","│    │    └─Sequential (conv)                 [1, 160, 8, 8]            320,000\n","│    └─InvertedResidual (16)                  [1, 160, 8, 8]            --\n","│    │    └─Sequential (conv)                 [1, 160, 8, 8]            320,000\n","│    └─InvertedResidual (17)                  [1, 320, 8, 8]            --\n","│    │    └─Sequential (conv)                 [1, 320, 8, 8]            473,920\n","│    └─Conv2dNormActivation (18)              [1, 1280, 8, 8]           --\n","│    │    └─Conv2d (0)                        [1, 1280, 8, 8]           409,600\n","│    │    └─BatchNorm2d (1)                   [1, 1280, 8, 8]           2,560\n","│    │    └─ReLU6 (2)                         [1, 1280, 8, 8]           --\n","├─Sequential (classifier)                     [1, 1000]                 --\n","│    └─Dropout (0)                            [1, 1280]                 --\n","│    └─Linear (1)                             [1, 1000]                 1,281,000\n","===============================================================================================\n","Total params: 3,504,872\n","Trainable params: 3,504,872\n","Non-trainable params: 0\n","Total mult-adds (M): 392.49\n","===============================================================================================\n","Input size (MB): 0.79\n","Forward/backward pass size (MB): 139.57\n","Params size (MB): 14.02\n","Estimated Total Size (MB): 154.37\n","===============================================================================================\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itMgm2C7n_EB"},"outputs":[],"source":["from torchinfo import summary\n","summary(backbone, input_size=(1, 3, 256, 256), row_settings=[\"var_names\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oBL1lfQn_EB"},"outputs":[],"source":["## Define your finetuned network consisting of MobileNet backbone and our custom classification layer\n","\n","###### YOUR CODE GOES HERE ######\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class ConvNetFinetuned(nn.Module):\n","    pass\n","#################################\n"]},{"cell_type":"markdown","metadata":{"id":"3ZVzDLXon_EB"},"source":["### Print a summary of the structure of our network using the `torchinfo` package. (**PROVIDED**)\n","\n","The result should look similar to:\n","```\n","====================================================================================================\n","Layer (type (var_name))                            Output Shape              Param #\n","====================================================================================================\n","ConvNetFinetuned (ConvNetFinetuned)                [1, 43]                   --\n","├─MobileNetV2 (model)                              --                        --\n","│    └─Sequential (features)                       [1, 1280, 8, 8]           --\n","│    │    └─Conv2dNormActivation (0)               [1, 32, 128, 128]         928\n","│    │    └─InvertedResidual (1)                   [1, 16, 128, 128]         896\n","│    │    └─InvertedResidual (2)                   [1, 24, 64, 64]           5,136\n","│    │    └─InvertedResidual (3)                   [1, 24, 64, 64]           8,832\n","│    │    └─InvertedResidual (4)                   [1, 32, 32, 32]           10,000\n","│    │    └─InvertedResidual (5)                   [1, 32, 32, 32]           14,848\n","│    │    └─InvertedResidual (6)                   [1, 32, 32, 32]           14,848\n","│    │    └─InvertedResidual (7)                   [1, 64, 16, 16]           21,056\n","│    │    └─InvertedResidual (8)                   [1, 64, 16, 16]           54,272\n","│    │    └─InvertedResidual (9)                   [1, 64, 16, 16]           54,272\n","│    │    └─InvertedResidual (10)                  [1, 64, 16, 16]           54,272\n","│    │    └─InvertedResidual (11)                  [1, 96, 16, 16]           66,624\n","│    │    └─InvertedResidual (12)                  [1, 96, 16, 16]           118,272\n","│    │    └─InvertedResidual (13)                  [1, 96, 16, 16]           118,272\n","│    │    └─InvertedResidual (14)                  [1, 160, 8, 8]            155,264\n","│    │    └─InvertedResidual (15)                  [1, 160, 8, 8]            320,000\n","│    │    └─InvertedResidual (16)                  [1, 160, 8, 8]            320,000\n","│    │    └─InvertedResidual (17)                  [1, 320, 8, 8]            473,920\n","│    │    └─Conv2dNormActivation (18)              [1, 1280, 8, 8]           412,160\n","│    └─Sequential (classifier)                     [1, 43]                   --\n","│    │    └─Dropout (0)                            [1, 1280]                 --\n","│    │    └─Linear (1)                             [1, 43]                   55,083\n","====================================================================================================\n","Total params: 2,278,955\n","Trainable params: 2,278,955\n","Non-trainable params: 0\n","Total mult-adds (M): 391.27\n","====================================================================================================\n","Input size (MB): 0.79\n","Forward/backward pass size (MB): 139.56\n","Params size (MB): 9.12\n","Estimated Total Size (MB): 149.46\n","====================================================================================================\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZqDqS-Bn_EB"},"outputs":[],"source":["# Instatiate our neural network\n","cnnFinetuned = ConvNetFinetuned()\n","\n","# Print a summary of the net\n","\n","from torchinfo import summary\n","summary(cnnFinetuned, input_size=(1, 3, 256, 256), row_settings=[\"var_names\"])"]},{"cell_type":"markdown","metadata":{"id":"rvYqnTOMn_EB"},"source":["### Neural Network Training (**add your code here**)\n","\n","Train your network using the `Trainer` class for 5 epochs using the same optimizer and loss as before:\n","\n","* [`torch.optim.AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?highlight=adamw#torch.optim.AdamW) is used as an optimizer with default parameters except for the learning rate which is set to `lr=1e-4`.\n","* [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) is used as loss function. Note, that the softmax activation is applied during loss computation, as stated in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n","* The number of training epochs is 3\n","\n","Train your multilayer perceptron network using the `Trainer` class and provide `trainLoader` as the DataLoader for training data and `valLoader` as the DataLoader for validation data.\n","\n","The overall training should take between 2-3 minutes per epoch (**on a GPU**, depending on what GPU is assigned). Reported accuracies on the training (validation) data should be > 99% (98%) after  training.   \n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzq1IgX1n_EB"},"outputs":[],"source":["##### YOUR CODE GOES HERE ######\n","\n","################################"]},{"cell_type":"markdown","metadata":{"id":"FQxLMLEmn_EB"},"source":["### Visualize the behavior of the loss and accuracy (**add your code here**)\n","\n","Using the data available in `trainer.metrics`, create the following two plots:\n","* Training loss and validation loss as a function of epochs.  \n","* Training accuracy and validation accuracy as a function of epochs.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsEnJEHBn_EB"},"outputs":[],"source":["##### YOUR CODE GOES HERE ######\n","\n","################################"]},{"cell_type":"markdown","metadata":{"id":"1ytcvFuTn_EB"},"source":["### Run your network on some images to get predictions (**PROVIDED**)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hOBfVTPn_EB"},"outputs":[],"source":["# Run some batches of unseen test data through the network and visualize its predictions\n","numBatches = 2\n","numRows = 8\n","\n"," # Iterator through the test DataLoader\n","dataIter = iter(testLoader)\n","\n","# for each batch\n","for batch in range(numBatches):\n","\n","    # get images and ground truth labels\n","    images, labels = next(dataIter)\n","\n","    # push to the device used\n","    images, labels = images.to(device), labels.to(device)\n","\n","    # forward pass of the batch of images\n","    outputs = cnnFinetuned(images)\n","\n","    # find the index of the class with the largest output\n","    _, predictedLabels         = torch.max(outputs, 1)\n","\n","    # print labels and outputs\n","    countCorrect = 0\n","    for i in range( batchSize // numRows ):\n","        for j in range((i*numRows), (i*numRows)+numRows):\n","            print(f'Image {j:2d} - Label: {classes[labels[j]]:5s} | Prediction: {classes[predictedLabels[j]]:5s}')\n","            if labels[j] == predictedLabels[j]:\n","                countCorrect = countCorrect + 1\n","\n","    print(f\"\\n{(countCorrect / batchSize) * 100.0:.2f}% of test images correctly classified\")\n","\n","    # show images\n","    imshow(torchvision.utils.make_grid(images))"]},{"cell_type":"markdown","metadata":{"id":"LtTaJ6sPn_EC"},"source":["### Evaluate the performance on the unseen test data set.  (**add your code here**)\n","\n","Use the proviced `Tester` class (see above) and test your trained network on the unseen test set available via `testLoader`. Your trained network should have approximately 97% accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNJ4ZOBCn_EC"},"outputs":[],"source":["###### YOUR CODE GOES HERE ######\n","\n","#################################"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"mlx-pytorch-2024-01","language":"python","name":"mlx-pytorch-2024-01"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}